## [ğŸ§± Databricks Lakehouse â€” Pipeline de IngestÃ£o e GovernanÃ§a de Dados](https://github.com/alevtelles/pipeline-de-ingestao-de-dados)
[![GitHub repo size](https://img.shields.io/github/repo-size/alevtelles/pipeline-de-ingestao-de-dados)](https://github.com/alevtelles/pipeline-de-ingestao-de-dados)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

Este repositÃ³rio contÃ©m a implementaÃ§Ã£o de uma arquitetura **Lakehouse** utilizando **Databricks**, **Apache Spark** e **Delta Lake**, com foco em **ingestÃ£o, transformaÃ§Ã£o e governanÃ§a de dados** atravÃ©s do **Unity Catalog** e versionamento integrado ao **GitHub**.

---

## ğŸš€ VisÃ£o Geral

Este projeto apresenta uma implementaÃ§Ã£o prÃ¡tica de arquitetura Lakehouse moderna, voltada Ã  ingestÃ£o e governanÃ§a de dados corporativos.

O objetivo Ã© demonstrar o ciclo completo de engenharia de dados â€” desde a ingestÃ£o de APIs atÃ© a curadoria e disponibilizaÃ§Ã£o dos dados para consumo analÃ­tico e aplicaÃ§Ãµes de InteligÃªncia Artificial â€” aplicando boas prÃ¡ticas de versionamento, qualidade e rastreabilidade.

O pipeline cobre:

- IngestÃ£o de dados de APIs pÃºblicas (ex: **Coinbase** para preÃ§o spot do Bitcoin);
- EstruturaÃ§Ã£o das camadas **Raw**, **Bronze**, **Silver** e **Gold** no **Unity Catalog**;
- AutomaÃ§Ã£o de cargas via **Jobs** e agendamento (cron);
- CriaÃ§Ã£o de camadas analÃ­ticas com **Spark** e **Delta Lake**;
- IntegraÃ§Ã£o com agentes de IA e dashboards de consumo.

---

## ğŸ¯ Objetivo

Estabelecer uma arquitetura Lakehouse **padronizada e auditÃ¡vel**, que permita:

- Centralizar dados provenientes de mÃºltiplas origens (APIs, bancos, etc.);
- Garantir **qualidade, rastreabilidade e versionamento** em todas as camadas;
- Possibilitar anÃ¡lises e automaÃ§Ãµes baseadas em **dados governados e confiÃ¡veis**;
- Integrar com **modelos de IA e agentes inteligentes** para insights automatizados.

---

## ğŸ—‚ï¸ Estrutura da Arquitetura

```text
Sistema de Origem (API / Banco de Dados)
        â†“
Camada Raw (dados brutos)
        â†“
Camada Bronze (dados limpos e padronizados)
        â†“
Camada Silver (modelagem e mÃ©tricas de negÃ³cio)
        â†“
Camada Gold (curadoria e consumo analÃ­tico)
        â†“
Agente de IA / BI / Dashboards
````

A gestÃ£o de seguranÃ§a e governanÃ§a Ã© feita pelo **Unity Catalog**, garantindo controle granular de acesso e lineage entre as camadas.

---

## âš™ï¸ Tecnologias e Ferramentas

| Categoria                | Ferramenta / ServiÃ§o                         |
| ------------------------ | -------------------------------------------- |
| Plataforma de Dados      | **Databricks (Community / Free Edition)**    |
| Engine de Processamento  | **Apache Spark 3.x**                         |
| Formato de Armazenamento | **Delta Lake**                               |
| Linguagens               | **Python**, **SQL**                          |
| GovernanÃ§a               | **Unity Catalog**                            |
| IngestÃ£o                 | **Requests (APIs)**, **Fivetran** (opcional) |
| Versionamento            | **Git e GitHub**                             |
| Armazenamento Simulado   | **Amazon S3 (via Volumes Databricks)**       |

---

## ğŸ§© Pipeline Implementado â€” IngestÃ£o da API da Coinbase

O script `notebooks/ingestao_bitcoin.py` implementa o fluxo de ingestÃ£o do preÃ§o spot do Bitcoin via API pÃºblica da Coinbase.

### Etapas do Pipeline:

1. CriaÃ§Ã£o do diretÃ³rio RAW no volume `/Volumes/lakehouse/raw/coinbase/coinbase/bitcoin_spot`;
2. Coleta de dados do endpoint `https://api.coinbase.com/v2/prices/spot?currency=USD`;
3. EstruturaÃ§Ã£o do DataFrame com metadados:

   * `ativo`, `preco`, `moeda`
   * `horario_coleta`, `source_system`, `source_endpoint`, `ingestion_ts_utc`
4. Escrita em formato **JSON (lines=True)** dentro da camada **Raw**;
5. ValidaÃ§Ã£o da ingestÃ£o com leitura via **Spark** e contagem de registros;
6. Agendamento de execuÃ§Ã£o recorrente via **Databricks Jobs (cron)**.

---

## ğŸ§  Boas PrÃ¡ticas Adotadas

* AplicaÃ§Ã£o da **Medallion Architecture (Bronze â†’ Silver â†’ Gold)**;
* SeparaÃ§Ã£o entre **coleta**, **transformaÃ§Ã£o** e **consumo**;
* InclusÃ£o de **metadados de rastreabilidade**:

  * `source_system`, `ingestion_ts_utc`, `source_endpoint`;
* **Versionamento GitHub** integrado ao Databricks;
* Uso de **Delta Tables** para versionamento temporal e histÃ³rico de alteraÃ§Ãµes;
* **Agendamento automÃ¡tico** de ingestÃµes via cron ou Jobs;
* GovernanÃ§a com **Unity Catalog** (controle por catÃ¡logo, schema e tabela).

---

## ğŸ“ˆ Resultados e BenefÃ­cios

* Pipelines **modulares, versionadas e auditÃ¡veis**;
* ReduÃ§Ã£o de falhas e retrabalho em processos manuais;
* Base sÃ³lida para camadas **analÃ­ticas e de IA**;
* Conformidade com boas prÃ¡ticas de **Data Governance**;
* IntegraÃ§Ã£o facilitada com **dashboards**, **APIs** e **agentes inteligentes**.

---

## âš¡ ExecuÃ§Ã£o e Setup

### PrÃ©-requisitos

* Conta gratuita no **Databricks Community Edition**;
* Conta no **GitHub** com acesso ao repositÃ³rio;
* Conhecimento bÃ¡sico em **Python** e **SQL**.

### Passos para executar

1. Clone este repositÃ³rio:

   ```bash
   git clone https://github.com/alevtelles/pipeline-de-ingestao-de-dados.git
   ```
2. Importe os notebooks no **Databricks Workspace**.
3. Configure o **Unity Catalog** e os **Volumes** conforme indicado nos notebooks.
4. Execute o notebook `ingestao_bitcoin.py`.
5. Valide a ingestÃ£o consultando os dados com:

   ```python
   df = spark.read.json("/Volumes/lakehouse/raw/coinbase/coinbase/bitcoin_spot")
   df.display()
   ```
6. Agende a execuÃ§Ã£o periÃ³dica via **Jobs** no Databricks.

---

## ğŸ“‚ Estrutura de DiretÃ³rios

```text
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ ingestao_bitcoin.py
â”‚   â”œâ”€â”€ transformacoes_silver.sql
â”‚   â”œâ”€â”€ agregacoes_gold.sql
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ arquitetura.md
â”‚   â”œâ”€â”€ configuracao_unity_catalog.md
â”‚
â”œâ”€â”€ volumes/
â”‚   â””â”€â”€ lakehouse/
â”‚       â”œâ”€â”€ raw/
â”‚       â”œâ”€â”€ bronze/
â”‚       â”œâ”€â”€ silver/
â”‚       â””â”€â”€ gold/
â”‚
â””â”€â”€ README.md
```

---

## ğŸ‘¤ Autor

**Engenheiro de Dados:** Alexsander Valente
**GitHub:** [https://github.com/alevtelles](https://github.com/alevtelles)
**LinkedIn:** [https://linkedin.com/in/alexsandervalente](https://linkedin.com/in/alexsandervalente)



